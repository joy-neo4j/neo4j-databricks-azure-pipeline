name: 4 - Integration (Neo4j_DBX) 

on:
  workflow_dispatch:
    inputs:
      skip_ingestion:
        description: "Skip CSV ingestion (expects Bronze to exist in selected catalog)"
        type: boolean
        default: true

permissions:
  contents: read
  id-token: write

concurrency:
  group: showcase-${{ github.ref }}
  cancel-in-progress: true

jobs:
  execute-pipeline:
    name: Execute Complete E-commerce Pipeline
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Databricks CLI (Go) and Python SDK
        run: |
          set -euo pipefail
          pip install databricks-sdk neo4j
          curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
          echo "$HOME/.databricks/bin" >> $GITHUB_PATH

      - name: Export Databricks env to all steps
        run: |
          echo "DATABRICKS_HOST=${{ secrets.DATABRICKS_HOST }}" >> $GITHUB_ENV
          echo "DATABRICKS_TOKEN=${{ secrets.DATABRICKS_TOKEN }}" >> $GITHUB_ENV

      # - name: Configure Jobs API version
      #   run: |
      #     databricks jobs configure --version=2.1

      - name: Verify Databricks CLI auth (optional)
        run: |
          databricks auth describe || true

      - name: Upload configs to DBFS
        run: |
          set -euo pipefail
          databricks fs mkdirs dbfs:/FileStore/configs || true
          databricks fs cp configs/data-sources.yml dbfs:/FileStore/configs/data-sources.yml --overwrite
          databricks fs cp configs/cluster-configurations.yml dbfs:/FileStore/configs/cluster-configurations.yml --overwrite
          # Removed: monitoring-config.yml and secrets-config.yml uploads (no longer used)
          databricks fs mkdirs dbfs:/FileStore/sample-data || true
          databricks fs cp sample-data/customers.csv dbfs:/FileStore/sample-data/customers.csv --overwrite
          databricks fs cp sample-data/products.csv dbfs:/FileStore/sample-data/products.csv --overwrite
          databricks fs cp sample-data/orders.csv dbfs:/FileStore/sample-data/orders.csv --overwrite
          databricks fs cp sample-data/reviews.csv dbfs:/FileStore/sample-data/reviews.csv --overwrite
          databricks fs cp sample-data/suppliers.csv dbfs:/FileStore/sample-data/suppliers.csv --overwrite
          databricks fs cp sample-data/categories.csv dbfs:/FileStore/sample-data/categories.csv --overwrite

      - name: Upload notebooks to Databricks workspace
        run: |
          set -euo pipefail
          databricks workspace mkdirs /Shared/neo4j-pipeline || true
          databricks workspace import /Shared/neo4j-pipeline/csv-ingestion --file databricks/notebooks/csv-ingestion.py --language PYTHON --format SOURCE --overwrite
          databricks workspace import /Shared/neo4j-pipeline/data-validation --file databricks/notebooks/data-validation.py --language PYTHON --format SOURCE --overwrite
          databricks workspace import /Shared/neo4j-pipeline/graph-transformation --file databricks/notebooks/graph-transformation.py --language PYTHON --format SOURCE --overwrite
          databricks workspace import /Shared/neo4j-pipeline/neo4j-loading --file databricks/notebooks/neo4j-loading.py --language PYTHON --format SOURCE --overwrite

      - name: Run E-commerce Data Ingestion
        if: ${{ !inputs.skip_ingestion }}
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          CATALOG_NAME: dbx_poc
        run: |
          cat > /tmp/run-ingestion.py << 'EOF'
          from databricks.sdk import WorkspaceClient
          import subprocess, json, time, os

          JOB_NAME = "E-commerce Data Ingestion"

          def get_client():
            host = os.environ.get("DATABRICKS_HOST")
            token = os.environ.get("DATABRICKS_TOKEN")
            if not host or not token:
              raise ValueError("Missing DATABRICKS_HOST or DATABRICKS_TOKEN for Databricks SDK auth")
            return WorkspaceClient(host=host, token=token)

          def ensure_job():
            w = get_client()
            jobs = list(w.jobs.list(name=JOB_NAME))
            cat = os.environ.get("CATALOG_NAME", "neo4j_pipeline")
            payload = {
              "name": JOB_NAME,
              "timeout_seconds": 3600,
              "max_concurrent_runs": 1,
              "tasks": [
                {
                  "task_key": "csv_ingestion",
                  "description": "Ingest CSV data from storage",
                  "notebook_task": {
                    "notebook_path": "/Shared/neo4j-pipeline/csv-ingestion",
                    "base_parameters": {"catalog": cat}
                  },
                  "new_cluster": {
                    "spark_version": "13.3.x-scala2.12",
                    "node_type_id": "Standard_DS3_v2",
                    "num_workers": 2,
                    "spark_conf": {"spark.databricks.delta.preview.enabled": "true"}
                  },
                  "timeout_seconds": 1200
                }
              ]
            }
            with open("/tmp/ingestion-job.json", "w") as f:
                json.dump(payload, f)

            if jobs:
                job_id = jobs[0].job_id
                reset_body = {"job_id": job_id, "new_settings": payload}
                with open("/tmp/ingestion-reset.json", "w") as f:
                    json.dump(reset_body, f)
                # Go CLI: pass the entire request via --json
                subprocess.run(
                    ["databricks", "jobs", "reset", "--json", "@/tmp/ingestion-reset.json"],
                    check=True
                )
                return list(get_client().jobs.list(name=JOB_NAME))[0]
            else:
                subprocess.run(
                    ["databricks", "jobs", "create", "--json", "@/tmp/ingestion-job.json"],
                    check=True
                )
                return list(get_client().jobs.list(name=JOB_NAME))[0]

          def write_summary(job_id: int, run_id: int, state: str, result: str, details: str = "", state_msg: str = ""):
            try:
              with open("/tmp/ingestion-summary.txt", "w", encoding="utf-8") as f:
                f.write("Ingestion Job Summary\n")
                f.write("=====================\n")
                host = os.environ.get("DATABRICKS_HOST")
                if host:
                  f.write(f"Run URL: {host}#job/{job_id}/run/{run_id}\n")
                f.write(f"Run ID: {run_id}\n")
                f.write(f"State: {state}\n")
                f.write(f"Result: {result}\n")
                if state_msg:
                  f.write(f"State Message: {state_msg}\n")
                if details:
                  f.write("\nDetails:\n")
                  f.write(details + "\n")
            except Exception as e:
              print(f"Failed to write summary: {e}")

          def run_job(job):
            w = get_client()
            print(f"Running ingestion job: {job.settings.name}")
            run = w.jobs.run_now(job_id=job.job_id)
            print(f"Job run started: {run.run_id}")
            start_ts = time.time(); max_wait_seconds = 1500  # Safety timeout (25m); GH job has 30m cap
            while True:
              run_status = w.jobs.get_run(run.run_id)
              def _norm(x):
                if x is None:
                  return None
                try:
                  v = getattr(x, "value", None)
                  s = str(v if v is not None else x)
                except Exception:
                  s = str(x)
                return s.split(".")[-1].upper()

              state = _norm(getattr(run_status.state, "life_cycle_state", None))
              state_msg = getattr(run_status.state, "state_message", "")
              print(f"Status: {state} | {state_msg}")
              # Prefer task-level state/result for faster detection
              task_result_state = None
              task_life_cycle_state = None
              try:
                for t in getattr(run_status, "tasks", []) or []:
                  task_life_cycle_state = _norm(getattr(getattr(t, "state", None), "life_cycle_state", None))
                  task_result_state = _norm(getattr(getattr(t, "state", None), "result_state", None))
                  break
              except Exception:
                pass

              result_state = _norm(getattr(run_status.state, "result_state", None))
              # Treat any explicit result state as terminal
              terminal_results = ["SUCCESS", "FAILED", "TIMEDOUT", "CANCELED"]
              if (task_result_state in terminal_results) or (result_state in terminal_results):
                details = ""
                try:
                  out = w.jobs.get_run_output(run.run_id)
                  if getattr(out, "notebook_output", None) and getattr(out.notebook_output, "result", None):
                    details = out.notebook_output.result
                  elif getattr(out, "error", None):
                    details = out.error
                except Exception as e:
                  details = f"No run output available: {e}"
                effective_result = task_result_state or result_state or ""
                text = (details or "")
                text_lower = text.lower()
                if ("failed:" in text_lower) or ("error" in text_lower):
                  effective_result = "FAILED"
                elif ("success:" in text_lower) and not effective_result:
                  effective_result = "SUCCESS"
                if effective_result == "SUCCESS":
                  write_summary(job.job_id, run.run_id, state or (task_life_cycle_state or ""), effective_result, details, state_msg)
                  print("✅ Ingestion completed successfully")
                  break
                else:
                  write_summary(job.job_id, run.run_id, state or (task_life_cycle_state or ""), effective_result, details, state_msg)
                  print(f"❌ Ingestion failed: {effective_result or 'FAILED'}")
                  raise SystemExit(1)
              # Global terminal life-cycle states
              if state in ["TERMINATED", "SKIPPED", "INTERNAL_ERROR"]:
                details = ""
                try:
                  out = w.jobs.get_run_output(run.run_id)
                  if getattr(out, "notebook_output", None) and getattr(out.notebook_output, "result", None):
                    details = out.notebook_output.result
                  elif getattr(out, "error", None):
                    details = out.error
                except Exception as e:
                  details = f"No run output available: {e}"
                effective_result = result_state or task_result_state or ""
                text = (details or "")
                text_lower = text.lower()
                if ("failed:" in text_lower) or ("error" in text_lower):
                  effective_result = "FAILED"
                elif ("success:" in text_lower) and not effective_result:
                  effective_result = "SUCCESS"
                if effective_result == "SUCCESS":
                  write_summary(job.job_id, run.run_id, state, effective_result, details, state_msg)
                  print("✅ Ingestion completed successfully")
                  break
                else:
                  write_summary(job.job_id, run.run_id, state, effective_result, details, state_msg)
                  print(f"❌ Ingestion failed: {effective_result or 'FAILED'}")
                  raise SystemExit(1)
              # If the task itself has finished but cluster is still terminating, allow early exit on success
              if (task_life_cycle_state in ["TERMINATED", "SKIPPED"]) and (task_result_state == "SUCCESS"):
                write_summary(job.job_id, run.run_id, task_life_cycle_state, task_result_state, "Task completed; cluster still terminating")
                print("✅ Ingestion task completed; exiting early before cluster termination")
                break

              time.sleep(10)
              if (time.time() - start_ts) > max_wait_seconds:
                print("❌ Ingestion monitoring timed out")
                raise SystemExit(1)

          job = ensure_job()
          run_job(job)
          EOF
          # Fail the workflow if ingestion job fails
          python /tmp/run-ingestion.py

      - name: Upload Ingestion Summary
        if: ${{ always() && !inputs.skip_ingestion }}
        uses: actions/upload-artifact@v4
        with:
          name: ingestion-summary
          path: /tmp/ingestion-summary.txt
          if-no-files-found: ignore

      - name: Append Ingestion Summary to Job Summary
        if: ${{ always() && !inputs.skip_ingestion }}
        run: |
          if [ -f /tmp/ingestion-summary.txt ]; then
            echo "### Ingestion Summary" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            sed 's/^/    /' /tmp/ingestion-summary.txt >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            RUN_URL=$(grep -E '^Run URL:' /tmp/ingestion-summary.txt | awk -F': ' '{print $2}')
            if [ -n "$RUN_URL" ]; then
              echo "[Open Databricks run]($RUN_URL)" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "(No ingestion summary file found)" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Run Graph Transformation and Neo4j Loading
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          # Set the catalog you want to use consistently across all tasks:
          CATALOG_NAME: dbx_poc
        run: |
          cat > /tmp/run-neo4j-loading.py << 'EOF'
          from databricks.sdk import WorkspaceClient
          import subprocess, json, time, os

          JOB_NAME = "Neo4j Graph Loading"

          def get_client():
            host = os.environ.get("DATABRICKS_HOST")
            token = os.environ.get("DATABRICKS_TOKEN")
            if not host or not token:
              raise ValueError("Missing DATABRICKS_HOST or DATABRICKS_TOKEN for Databricks SDK auth")
            return WorkspaceClient(host=host, token=token)

          def ensure_job():
            w = get_client()
            jobs = list(w.jobs.list(name=JOB_NAME))
            cat = os.environ.get("CATALOG_NAME", "neo4j_pipeline")
            payload = {
              "name": JOB_NAME,
              "timeout_seconds": 3600,
              "max_concurrent_runs": 1,
              "tasks": [
                {
                  "task_key": "data_validation",
                  "description": "Validate and promote data to Silver",
                  "notebook_task": {
                    "notebook_path": "/Shared/neo4j-pipeline/data-validation",
                    "base_parameters": {"catalog": cat}
                  },
                  "new_cluster": {
                    "spark_version": "13.3.x-scala2.12",
                    "node_type_id": "Standard_DS3_v2",
                    "num_workers": 2
                  },
                  "timeout_seconds": 1200
                },
                {
                  "task_key": "graph_transformation",
                  "description": "Transform data to graph-ready format",
                  "depends_on": [{"task_key": "data_validation"}],
                  "notebook_task": {
                    "notebook_path": "/Shared/neo4j-pipeline/graph-transformation",
                    "base_parameters": {"catalog": cat}
                  },
                  "new_cluster": {
                    "spark_version": "13.3.x-scala2.12",
                    "node_type_id": "Standard_DS3_v2",
                    "num_workers": 2
                  },
                  "timeout_seconds": 1200
                },
                {
                  "task_key": "neo4j_loading",
                  "description": "Load data to Neo4j Aura",
                  "depends_on": [{"task_key": "graph_transformation"}],
                  "notebook_task": {
                    "notebook_path": "/Shared/neo4j-pipeline/neo4j-loading",
                    "base_parameters": {"batch_size": "1000", "catalog": cat}
                  },
                  "new_cluster": {
                    "spark_version": "13.3.x-scala2.12",
                    "node_type_id": "Standard_DS3_v2",
                    "num_workers": 2,
                    "spark_conf": {"spark.databricks.delta.preview.enabled": "true"}
                  },
                  "timeout_seconds": 1800
                }
              ]
            }
            
            with open("/tmp/neo4j-loading-job.json", "w") as f:
                json.dump(payload, f)

            if jobs:
                job_id = jobs[0].job_id
                reset_body = {"job_id": job_id, "new_settings": payload}
                with open("/tmp/neo4j-loading-reset.json", "w") as f:
                    json.dump(reset_body, f)
                subprocess.run(
                    ["databricks", "jobs", "reset", "--json", "@/tmp/neo4j-loading-reset.json"],
                    check=True
                )
                return list(get_client().jobs.list(name=JOB_NAME))[0]
            else:
                subprocess.run(
                    ["databricks", "jobs", "create", "--json", "@/tmp/neo4j-loading-job.json"],
                    check=True
                )
                return list(get_client().jobs.list(name=JOB_NAME))[0]

          def write_summary(job_id: int, run_id: int, state: str, result: str, details: str = "", state_msg: str = ""):
            try:
              with open("/tmp/loading-summary.txt", "w", encoding="utf-8") as f:
                f.write("Loading Job Summary\n")
                f.write("===================\n")
                host = os.environ.get("DATABRICKS_HOST")
                if host:
                  f.write(f"Run URL: {host}#job/{job_id}/run/{run_id}\n")
                f.write(f"Run ID: {run_id}\n")
                f.write(f"State: {state}\n")
                f.write(f"Result: {result}\n")
                if state_msg:
                  f.write(f"State Message: {state_msg}\n")
                if details:
                  f.write("\nDetails:\n")
                  f.write(details + "\n")
            except Exception as e:
              print(f"Failed to write summary: {e}")

          def run_job(job):
            w = get_client()
            print(f"Running Neo4j loading job: {job.settings.name}")
            run = w.jobs.run_now(job_id=job.job_id)
            print(f"Job run started: {run.run_id}")
            start_ts = time.time(); max_wait_seconds = 1800
            while True:
              run_status = w.jobs.get_run(run.run_id)
              def _norm(x):
                if x is None:
                  return None
                try:
                  v = getattr(x, "value", None)
                  s = str(v if v is not None else x)
                except Exception:
                  s = str(x)
                return s.split(".")[-1].upper()

              state = _norm(getattr(run_status.state, "life_cycle_state", None))
              state_msg = getattr(run_status.state, "state_message", "")
              print(f"Status: {state} | {state_msg}")

              task_result_state = None
              task_life_cycle_state = None
              try:
                for t in getattr(run_status, "tasks", []) or []:
                  task_life_cycle_state = _norm(getattr(getattr(t, "state", None), "life_cycle_state", None))
                  task_result_state = _norm(getattr(getattr(t, "state", None), "result_state", None))
              except Exception:
                pass

              result_state = _norm(getattr(run_status.state, "result_state", None))
              terminal_results = ["SUCCESS", "FAILED", "TIMEDOUT", "CANCELED"]

              if (task_result_state in terminal_results) or (result_state in terminal_results):
                details = ""
                try:
                  out = None
                  try:
                    out = w.jobs.get_run_output(run.run_id, task_key="neo4j_loading")
                  except Exception:
                    out = w.jobs.get_run_output(run.run_id)
                  if getattr(out, "notebook_output", None) and getattr(out.notebook_output, "result", None):
                    details = out.notebook_output.result
                  elif getattr(out, "error", None):
                    details = out.error
                except Exception as e:
                  details = f"No run output available: {e}"

                effective_result = task_result_state or result_state or ""
                text = (details or "")
                text_lower = text.lower()
                if ("failed:" in text_lower) or ("error" in text_lower):
                  effective_result = "FAILED"
                elif ("success:" in text_lower) and not effective_result:
                  effective_result = "SUCCESS"

                if effective_result == "SUCCESS":
                  write_summary(job.job_id, run.run_id, state or (task_life_cycle_state or ""), effective_result, details, state_msg)
                  print("✅ Neo4j loading completed successfully")
                  break
                else:
                  write_summary(job.job_id, run.run_id, state or (task_life_cycle_state or ""), effective_result, details, state_msg)
                  print(f"❌ Neo4j loading failed: {effective_result or 'FAILED'}")
                  raise SystemExit(1)

              if state in ["TERMINATED", "SKIPPED", "INTERNAL_ERROR"]:
                details = ""
                try:
                  out = None
                  try:
                    out = w.jobs.get_run_output(run.run_id, task_key="neo4j_loading")
                  except Exception:
                    out = w.jobs.get_run_output(run.run_id)
                  if getattr(out, "notebook_output", None) and getattr(out.notebook_output, "result", None):
                    details = out.notebook_output.result
                  elif getattr(out, "error", None):
                    details = out.error
                except Exception as e:
                  details = f"No run output available: {e}"

                effective_result = result_state or task_result_state or ""
                text = (details or "")
                text_lower = text.lower()
                if ("failed:" in text_lower) or ("error" in text_lower):
                  effective_result = "FAILED"
                elif ("success:" in text_lower) and not effective_result:
                  effective_result = "SUCCESS"

                if effective_result == "SUCCESS":
                  write_summary(job.job_id, run.run_id, state, effective_result, details, state_msg)
                  print("✅ Neo4j loading completed successfully")
                  break
                else:
                  write_summary(job.job_id, run.run_id, state, effective_result, details, state_msg)
                  print(f"❌ Neo4j loading failed: {effective_result or 'FAILED'}")
                  raise SystemExit(1)

              if (task_life_cycle_state in ["TERMINATED", "SKIPPED"]) and (task_result_state == "SUCCESS"):
                write_summary(job.job_id, run.run_id, task_life_cycle_state, task_result_state, "Tasks completed; cluster still terminating", state_msg)
                print("✅ Loading tasks completed; exiting early before cluster termination")
                break

              time.sleep(10)
              if (time.time() - start_ts) > max_wait_seconds:
                print("❌ Loading monitoring timed out")
                raise SystemExit(1)

          job = ensure_job()
          run_job(job)
          EOF
          python /tmp/run-neo4j-loading.py
      
      - name: Upload Loading Summary
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: loading-summary
          path: /tmp/loading-summary.txt
          if-no-files-found: ignore

      - name: Append Loading Summary to Job Summary
        if: ${{ always() }}
        run: |
          if [ -f /tmp/loading-summary.txt ]; then
            echo "### Loading Summary" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            sed 's/^/    /' /tmp/loading-summary.txt >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            RUN_URL=$(grep -E '^Run URL:' /tmp/loading-summary.txt | awk -F': ' '{print $2}')
            if [ -n "$RUN_URL" ]; then
              echo "[Open Databricks run]($RUN_URL)" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "(No loading summary file found)" >> $GITHUB_STEP_SUMMARY
          fi

  verify-graph-data:
    name: Verify Neo4j Graph Data
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: execute-pipeline
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Neo4j driver
        run: |
          pip install neo4j

      - name: Verify Graph Structure
        env:
          NEO4J_URI: ${{ secrets.NEO4J_URI }}
          NEO4J_USER: ${{ secrets.NEO4J_USERNAME }}     # aligned to secret name
          NEO4J_PASSWORD: ${{ secrets.NEO4J_PASSWORD }}
        run: |
          set -euo pipefail
          # Skip if any secrets are missing
          if [ -z "${NEO4J_URI:-}" ] || [ -z "${NEO4J_USER:-}" ] || [ -z "${NEO4J_PASSWORD:-}" ]; then
            echo "::warning::Skipping Neo4j verification because one or more secrets are not set (NEO4J_URI/NEO4J_USERNAME/NEO4J_PASSWORD)."
            exit 0
          fi

          cat > /tmp/verify-graph.py << 'EOF'
          from neo4j import GraphDatabase
          import os, sys
          uri = os.environ['NEO4J_URI']; user = os.environ['NEO4J_USER']; password = os.environ['NEO4J_PASSWORD']
          try:
              driver = GraphDatabase.driver(uri, auth=(user, password))
              with driver.session() as session:
                  result = session.run("RETURN 1 as test"); assert result.single()["test"] == 1
                  print("✅ Neo4j connection successful")
              driver.close()
          except Exception as e:
              print(f"❌ Failed to verify graph: {str(e)}"); sys.exit(1)
          EOF
          python /tmp/verify-graph.py
